<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ethics on Bram Steensma</title>
    <link>http://localhost:1313/tags/ethics/</link>
    <description>Recent content in Ethics on Bram Steensma</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 27 Feb 2026 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/ethics/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Policing the Future: When Algorithms Carry a Badge and Bias</title>
      <link>http://localhost:1313/blog/policing-the-future/</link>
      <pubDate>Fri, 27 Feb 2026 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/blog/policing-the-future/</guid>
      <description>&lt;p&gt;Artificial intelligence in law enforcement was sold with a powerful promise: the ability to stop crime before it happens. This proactive approach, shifting from reaction to prediction, presented an alluring vision of safer streets and optimized police resources. For years, tech companies championed predictive policing systems as the key to smarter, data-driven public safety.&lt;/p&gt;&#xA;&lt;p&gt;However, as cities across the globe have discovered, the line between science fiction and reality is fraught with ethical peril. The deployment of these algorithms has become a crucial case study in the challenges of AI ethics. When we examine these systems through the lens of ethical AI frameworks, particularly the concepts outlined by Luciano Floridi, we see a fundamental conflict between computational logic and human justice. The real-world retreat from these technologies, most notably in cities like Los Angeles, signals a critical re-evaluation of what it means to police fairly in the digital age.&lt;/p&gt;</description>
    </item>
    <item>
      <title>When Anonymous Isn&#39;t Really Anonymous: A Data Ethics Crisis</title>
      <link>http://localhost:1313/blog/when-anonymous-isnt-really-anonymous/</link>
      <pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/blog/when-anonymous-isnt-really-anonymous/</guid>
      <description>&lt;p&gt;Organizations depend on course feedback surveys to improve their training programs. This process seems straightforward: students take an “anonymous” survey, management reviews the results, and courses are updated based on feedback. But what happens when the promise of anonymity is undermined by the survey’s own design?&lt;/p&gt;&#xA;&lt;p&gt;Let’s examine a real-world scenario. In this organization, survey participants are assured their responses will remain anonymous. Yet, the structure of the survey reveals fundamental flaws that dissolve this promise. The implications reach beyond poor design—they erode trust, challenge consent, create data governance concerns, and put pressure on the ethical responsibilities of data professionals.&#xA;This isn’t just a theoretical dilemma; it’s the type of ethical puzzle data analysts regularly face, where technical systems can quietly clash with core ethical principles.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
